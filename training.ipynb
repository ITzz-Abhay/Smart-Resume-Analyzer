{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61dd3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('/Users/abhaysingh/Documents/Resume/data/Cleaned_resume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5301a5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 962 entries, 0 to 961\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Category        962 non-null    object\n",
      " 1   cleaned_resume  962 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.2+ KB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 962 entries, 0 to 961\n",
      "Series name: Category\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "962 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 7.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.info()\n",
    "df[\"Category\"].unique()\n",
    "df[\"Category\"].info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9987d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "df['Category']=le.fit_transform(df['Category'])\n",
    "df.head()\n",
    "X=df['cleaned_resume']\n",
    "y=df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8aed39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6b73dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(769, 5000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(max_features=5000)\n",
    "X_train_tf=tfidf.fit_transform(X_train)\n",
    "X_test_tf=tfidf.transform(X_test)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19e9a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80642550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  def __init__(self,features,labels):\n",
    "    self.features=torch.tensor(features,dtype=torch.float32)\n",
    "    self.labels=torch.tensor(labels,dtype=torch.long)\n",
    "  def __len__(self):\n",
    "    return len(self.features)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.features[idx],self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "016ca19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(769, 5000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as TF-IDF gives a sparce matrix but pytorch doesnt take in the form of matrix\n",
    "X_train=X_train_tf.toarray()\n",
    "X_test=X_test_tf.toarray()\n",
    "X_train.shape         # (769,5000). 769 ---> number of samples ,,,,,, 5000--> number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e90fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=MyDataset(X_train,y_train.values) # as y_train is a pandas series we have to make it into numpys arrays\n",
    "test_dataset=MyDataset(X_test,y_test.values)\n",
    "\n",
    "# now we are going to make our loaders so that we can give training in batches we use 32 batch\n",
    "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ac64477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "num_classes = df['Category'].nunique()\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22e05682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class MyNN(nn.Module):\n",
    "  def __init__(self,num_features):\n",
    "    super().__init__()\n",
    "\n",
    "    self.model=nn.Sequential(\n",
    "        nn.Linear(num_features,128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128,64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64,25)\n",
    "    )\n",
    "  def forward(self,X):\n",
    "    return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05538023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input TF-IDF vector (5000-d) \n",
    "#         ↓ Linear + ReLU\n",
    "# Hidden Layer 128-d\n",
    "#         ↓ Linear + ReLU\n",
    "# Hidden Layer 64-d\n",
    "#         ↓ Linear\n",
    "# Output Layer 25-d logits → predicted scores for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "774cf55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets decide the learning rate and epoch size\n",
    "l_rate=0.01\n",
    "epochs=100\n",
    "criterion=nn.CrossEntropyLoss() # loss function of the model\n",
    "model=MyNN(X_train.shape[1])  # instance of the model\n",
    "optimizer=optim.Adam(model.parameters(),lr=l_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "beac74f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1,Loss:2.125615677833557\n",
      "Epoch2,Loss:0.12495484890416236\n",
      "Epoch3,Loss:0.0010120257321975146\n",
      "Epoch4,Loss:0.000263427312602289\n",
      "Epoch5,Loss:0.00010071021313706296\n",
      "Epoch6,Loss:7.746503109956393e-05\n",
      "Epoch7,Loss:6.388268357113703e-05\n",
      "Epoch8,Loss:7.629207895661239e-05\n",
      "Epoch9,Loss:5.3890456183580684e-05\n",
      "Epoch10,Loss:4.4173746791784653e-05\n",
      "Epoch11,Loss:3.745379166502971e-05\n",
      "Epoch12,Loss:3.400053306904738e-05\n",
      "Epoch13,Loss:3.1362008576252266e-05\n",
      "Epoch14,Loss:2.8955932448297973e-05\n",
      "Epoch15,Loss:2.9937638646515553e-05\n",
      "Epoch16,Loss:2.477666179402149e-05\n",
      "Epoch17,Loss:2.7503595265443437e-05\n",
      "Epoch18,Loss:2.1599045858238242e-05\n",
      "Epoch19,Loss:2.093196973874001e-05\n",
      "Epoch20,Loss:1.9275581908004825e-05\n",
      "Epoch21,Loss:1.7794986997614616e-05\n",
      "Epoch22,Loss:1.735806163196685e-05\n",
      "Epoch23,Loss:1.5897852753141704e-05\n",
      "Epoch24,Loss:1.505602513134363e-05\n",
      "Epoch25,Loss:1.4498048894893145e-05\n",
      "Epoch26,Loss:1.3630715980070818e-05\n",
      "Epoch27,Loss:1.3071236171526835e-05\n",
      "Epoch28,Loss:1.2476876772780088e-05\n",
      "Epoch29,Loss:1.2791022909368621e-05\n",
      "Epoch30,Loss:1.1475000901555177e-05\n",
      "Epoch31,Loss:1.1432564333517803e-05\n",
      "Epoch32,Loss:1.0330361965316115e-05\n",
      "Epoch33,Loss:1.0048158837889787e-05\n",
      "Epoch34,Loss:1.550004451928544e-05\n",
      "Epoch35,Loss:9.2386353026086e-06\n",
      "Epoch36,Loss:9.244468601536938e-06\n",
      "Epoch37,Loss:1.0664793189789633e-05\n",
      "Epoch38,Loss:7.959279905662697e-06\n",
      "Epoch39,Loss:7.75782957134652e-06\n",
      "Epoch40,Loss:7.348655403802696e-06\n",
      "Epoch41,Loss:7.1396039584215035e-06\n",
      "Epoch42,Loss:6.798376415417806e-06\n",
      "Epoch43,Loss:9.4125388750399e-06\n",
      "Epoch44,Loss:7.422136104651145e-06\n",
      "Epoch45,Loss:6.025177481205901e-06\n",
      "Epoch46,Loss:6.002683658152819e-06\n",
      "Epoch47,Loss:5.7110759189527015e-06\n",
      "Epoch48,Loss:5.5269045878958426e-06\n",
      "Epoch49,Loss:5.4018889022700024e-06\n",
      "Epoch50,Loss:5.113556462674751e-06\n",
      "Epoch51,Loss:4.985262885384145e-06\n",
      "Epoch52,Loss:4.813754858048469e-06\n",
      "Epoch53,Loss:4.894075536867604e-06\n",
      "Epoch54,Loss:5.033253473811783e-06\n",
      "Epoch55,Loss:4.951001851623005e-06\n",
      "Epoch56,Loss:4.366585626485175e-06\n",
      "Epoch57,Loss:4.951897835780983e-06\n",
      "Epoch58,Loss:4.0387658759755145e-06\n",
      "Epoch59,Loss:3.9004857171676125e-06\n",
      "Epoch60,Loss:3.827174323305371e-06\n",
      "Epoch61,Loss:3.7167587697695125e-06\n",
      "Epoch62,Loss:3.611260598290755e-06\n",
      "Epoch63,Loss:3.511424747557612e-06\n",
      "Epoch64,Loss:3.4401989660182154e-06\n",
      "Epoch65,Loss:3.58384694209235e-06\n",
      "Epoch66,Loss:3.30787972870894e-06\n",
      "Epoch67,Loss:3.2320342825187253e-06\n",
      "Epoch68,Loss:5.201613998906396e-06\n",
      "Epoch69,Loss:2.9720135759703227e-06\n",
      "Epoch70,Loss:2.9027243169821303e-06\n",
      "Epoch71,Loss:3.7356895199991413e-06\n",
      "Epoch72,Loss:2.7547578247322233e-06\n",
      "Epoch73,Loss:2.6930680303394183e-06\n",
      "Epoch74,Loss:2.6859163381232066e-06\n",
      "Epoch75,Loss:2.5668568738979046e-06\n",
      "Epoch76,Loss:2.5637282897150726e-06\n",
      "Epoch77,Loss:2.5129160667347606e-06\n",
      "Epoch78,Loss:2.41650625184775e-06\n",
      "Epoch79,Loss:2.5631336711740004e-06\n",
      "Epoch80,Loss:2.3861090403443084e-06\n",
      "Epoch81,Loss:2.2894012249707885e-06\n",
      "Epoch82,Loss:2.193140297777063e-06\n",
      "Epoch83,Loss:2.1608053498312074e-06\n",
      "Epoch84,Loss:2.0940486410836457e-06\n",
      "Epoch85,Loss:2.092559004722716e-06\n",
      "Epoch86,Loss:2.0019604153276304e-06\n",
      "Epoch87,Loss:1.962323703992297e-06\n",
      "Epoch88,Loss:1.9359490261194877e-06\n",
      "Epoch89,Loss:2.125044434251322e-06\n",
      "Epoch90,Loss:1.8480329978842747e-06\n",
      "Epoch91,Loss:2.1532075334107505e-06\n",
      "Epoch92,Loss:1.759669889906945e-06\n",
      "Epoch93,Loss:1.815400532905187e-06\n",
      "Epoch94,Loss:1.7005129359404237e-06\n",
      "Epoch95,Loss:1.6599821103113753e-06\n",
      "Epoch96,Loss:1.624368601369497e-06\n",
      "Epoch97,Loss:1.5991859129371734e-06\n",
      "Epoch98,Loss:1.6254121578640478e-06\n",
      "Epoch99,Loss:1.5340683250997244e-06\n",
      "Epoch100,Loss:1.4924943616279052e-06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_epoch_loss=0\n",
    "    for batch_features,batch_labels in train_loader:\n",
    "\n",
    "        # forward pass\n",
    "        num_predict=model(batch_features)\n",
    "\n",
    "        # loss\n",
    "        loss=criterion(num_predict,batch_labels)\n",
    "\n",
    "        #backpropagate with zero grad\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #optimize.step() update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate the loss \n",
    "        total_epoch_loss=total_epoch_loss+loss.item()\n",
    "  \n",
    "    avg_loss=total_epoch_loss/len(train_loader)\n",
    "    print(f\"Epoch{epoch+1},Loss:{avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e50f63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=5000, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=25, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a32e200f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.48186528497409%\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "total = 0        # Total number of test samples\n",
    "correct = 0      # Total number of correct predictions\n",
    "\n",
    "# Disable gradient calculation during evaluation\n",
    "# This makes evaluation faster and uses less memory\n",
    "with torch.no_grad():  \n",
    "    # Loop over the test dataset in batches\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        \n",
    "        # Forward pass: get model outputs (logits for each class)\n",
    "        pred_output = model(batch_features)\n",
    "        \n",
    "        # Get predicted class for each sample\n",
    "        # torch.max returns (max_value, index_of_max)\n",
    "        # We only need the index which corresponds to predicted class\n",
    "        _, predicted = torch.max(pred_output, dim=1)\n",
    "        \n",
    "        # Update total number of samples seen\n",
    "        total += batch_labels.size(0)\n",
    "        \n",
    "        # Count correct predictions in this batch and update total correct\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "# Calculate overall accuracy as percentage\n",
    "accuracy_score = (correct / total) * 100\n",
    "\n",
    "# Print test accuracy\n",
    "print(f\"Accuracy: {accuracy_score}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c4903cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we achieve a good accuracy now it the time to save the model\n",
    "torch.save(model.state_dict(),\"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc8a03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are now going to save the vectorized resumes in the pickle file \n",
    "import pickle\n",
    "with open(\"tfidf_vectorizer.pkl\",\"wb\") as f:\n",
    "    pickle.dump(tfidf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edcb4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Label_encoding.pkl\",\"wb\") as f:\n",
    "    pickle.dump(le,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
